{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Latent Diffusion LAION-400M model text-to-image",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text-to-image synthesis with 1.45B parameter LAION-400M trained Latent Diffusion model\n",
        "### Model by CompVis. Notebook assembled by @multimodalart"
      ],
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all the requirements"
      ],
      "metadata": {
        "id": "zh7u8gOx0ivw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHgUAp48qwoG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "!git clone https://github.com/CompVis/latent-diffusion.git\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "!pip install transformers\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('./taming-transformers')\n",
        "from taming.models import vqgan "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, download the checkpoint (~5.7 GB). This will usually take 3-6 minutes."
      ],
      "metadata": {
        "id": "fNqCqQDoyZmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download model\n",
        "%cd latent-diffusion/ \n",
        "\n",
        "!mkdir -p models/ldm/text2img-large/\n",
        "!wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt"
      ],
      "metadata": {
        "id": "cNHvQBhzyXCI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also check what type of GPU we've got."
      ],
      "metadata": {
        "id": "ThxmCePqt1mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "jbL2zJ7Pt7Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load it."
      ],
      "metadata": {
        "id": "1tWAqdwk0Nrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    config = OmegaConf.load(\"configs/latent-diffusion/cin256-v2.yaml\")  \n",
        "    model = load_model_from_config(config, \"models/ldm/cin256-v2/model.ckpt\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "fnGwQRhtyBhb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import stuff\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import transformers\n",
        "import gc\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler"
      ],
      "metadata": {
        "id": "BPnyd-XUKbfE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load necessary functions\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def run(opt):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\")  # TODO: Optionally download from same location as ckpt and chnage this logic\n",
        "    model = load_model_from_config(config, \"models/ldm/text2img-large/model.ckpt\")  # TODO: check path\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model = model.to(device)\n",
        "    sampler = DDIMSampler(model)\n",
        "\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    prompt = opt.prompt\n",
        "\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "\n",
        "    all_samples=list()\n",
        "    with torch.no_grad():\n",
        "        with model.ema_scope():\n",
        "            uc = None\n",
        "            if opt.scale > 0:\n",
        "                uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
        "            for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                c = model.get_learned_conditioning(opt.n_samples * [prompt])\n",
        "                shape = [4, opt.H//8, opt.W//8]\n",
        "                samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                 conditioning=c,\n",
        "                                                 batch_size=opt.n_samples,\n",
        "                                                 shape=shape,\n",
        "                                                 verbose=False,\n",
        "                                                 unconditional_guidance_scale=opt.scale,\n",
        "                                                 unconditional_conditioning=uc,\n",
        "                                                 eta=opt.ddim_eta)\n",
        "\n",
        "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "                for x_sample in x_samples_ddim:\n",
        "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                    Image.fromarray(x_sample.astype(np.uint8)).save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
        "                    base_count += 1\n",
        "                all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "    # additionally, save as grid\n",
        "    grid = torch.stack(all_samples, 0)\n",
        "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "    grid = make_grid(grid, nrow=opt.n_samples)\n",
        "\n",
        "    # to image\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    \n",
        "    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))\n",
        "    display(Image.fromarray(grid.astype(np.uint8)))\n",
        "    #print(f\"Your samples are ready and waiting four you here: \\n{outpath} \\nEnjoy.\")"
      ],
      "metadata": {
        "id": "HY_7vvnPThzS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "args = argparse.Namespace(\n",
        "    prompt = 'a robot holding a sign that reads: \"This is weird\"',\n",
        "    outdir='outputs',\n",
        "    ddim_steps = 50,\n",
        "    ddim_eta = 0,\n",
        "    n_iter = 2,\n",
        "    W=256,\n",
        "    H=256,\n",
        "    n_samples=4,\n",
        "    scale=5.0\n",
        ")\n",
        "run(args)"
      ],
      "metadata": {
        "id": "fmafGmcyT1mZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}